
import os
from datetime import datetime, timedelta
import json
import hashlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import IsolationForest
from sklearn.svm import SVR

import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA

import streamlit as st
import plotly.graph_objects as go
import plotly.express as px

# --- Optional deps (graceful fallback)
try:
    from scipy.stats import ttest_ind, mannwhitneyu
    HAS_SCIPY = True
except Exception:
    HAS_SCIPY = False

try:
    from streamlit_autorefresh import st_autorefresh
    HAS_AUTOREFRESH = True
except Exception:
    HAS_AUTOREFRESH = False

# Add import for OpenAI API
try:
    from openai import OpenAI
    HAS_OPENAI = True
except Exception:
    HAS_OPENAI = False


# =======================
# 1. Data Integration
# =======================
@st.cache_data(show_spinner=False)
def load_and_clean_data(accident_file, strategy_file):
    accident_df = pd.read_excel(accident_file, sheet_name=None)
    accident_data = pd.concat(accident_df.values(), ignore_index=True)

    accident_data['äº‹æ•…æ—¶é—´'] = pd.to_datetime(accident_data['äº‹æ•…æ—¶é—´'])
    accident_data = accident_data.dropna(subset=['äº‹æ•…æ—¶é—´', 'æ‰€åœ¨è¡—é“', 'äº‹æ•…ç±»å‹'])

    strategy_df = pd.read_excel(strategy_file)
    strategy_df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(strategy_df['å‘å¸ƒæ—¶é—´'])
    strategy_df = strategy_df.dropna(subset=['å‘å¸ƒæ—¶é—´', 'äº¤é€šç­–ç•¥ç±»å‹'])

    severity_map = {'è´¢æŸ': 1, 'ä¼¤äºº': 2, 'äº¡äºº': 4}
    accident_data['severity'] = accident_data['äº‹æ•…ç±»å‹'].map(severity_map).fillna(1)

    accident_data = accident_data[['äº‹æ•…æ—¶é—´', 'æ‰€åœ¨è¡—é“', 'äº‹æ•…ç±»å‹', 'severity']] \
        .rename(columns={'äº‹æ•…æ—¶é—´': 'date_time', 'æ‰€åœ¨è¡—é“': 'region', 'äº‹æ•…ç±»å‹': 'category'})
    strategy_df = strategy_df[['å‘å¸ƒæ—¶é—´', 'äº¤é€šç­–ç•¥ç±»å‹']] \
        .rename(columns={'å‘å¸ƒæ—¶é—´': 'date_time', 'äº¤é€šç­–ç•¥ç±»å‹': 'strategy_type'})

    return accident_data, strategy_df


@st.cache_data(show_spinner=False)
def aggregate_daily_data(accident_data: pd.DataFrame, strategy_data: pd.DataFrame) -> pd.DataFrame:
    # City-level aggregation
    accident_data = accident_data.copy()
    strategy_data = strategy_data.copy()

    accident_data['date'] = accident_data['date_time'].dt.date
    daily_accidents = accident_data.groupby('date').agg(
        accident_count=('date_time', 'count'),
        severity=('severity', 'sum')
    )
    daily_accidents.index = pd.to_datetime(daily_accidents.index)

    strategy_data['date'] = strategy_data['date_time'].dt.date
    daily_strategies = strategy_data.groupby('date')['strategy_type'].apply(list)
    daily_strategies.index = pd.to_datetime(daily_strategies.index)

    combined = daily_accidents.join(daily_strategies, how='left')
    combined['strategy_type'] = combined['strategy_type'].apply(lambda x: x if isinstance(x, list) else [])
    combined = combined.asfreq('D')
    combined[['accident_count', 'severity']] = combined[['accident_count', 'severity']].fillna(0)
    combined['strategy_type'] = combined['strategy_type'].apply(lambda x: x if isinstance(x, list) else [])
    return combined


@st.cache_data(show_spinner=False)
def aggregate_daily_data_by_region(accident_data: pd.DataFrame, strategy_data: pd.DataFrame) -> pd.DataFrame:
    """åŒºåŸŸç»´åº¦èšåˆã€‚ç­–ç•¥æŒ‰å¤©å¹¿æ’­åˆ°æ‰€æœ‰åŒºåŸŸï¼ˆè‹¥ç­–ç•¥æœ¬èº«æ— åŒºåŸŸå­—æ®µï¼‰ã€‚"""
    df = accident_data.copy()
    df['date'] = df['date_time'].dt.date
    g = df.groupby(['region', 'date']).agg(
        accident_count=('date_time', 'count'),
        severity=('severity', 'sum')
    )
    g.index = g.index.set_levels([g.index.levels[0], pd.to_datetime(g.index.levels[1])])
    g = g.sort_index()

    # ç­–ç•¥ï¼ˆæ¯æ—¥åˆ—è¡¨ï¼‰
    s = strategy_data.copy()
    s['date'] = s['date_time'].dt.date
    daily_strategies = s.groupby('date')['strategy_type'].apply(list)
    daily_strategies.index = pd.to_datetime(daily_strategies.index)

    # å¹¿æ’­
    regions = g.index.get_level_values(0).unique()
    dates = pd.date_range(g.index.get_level_values(1).min(), g.index.get_level_values(1).max(), freq='D')
    full_index = pd.MultiIndex.from_product([regions, dates], names=['region', 'date'])
    g = g.reindex(full_index).fillna(0)

    strat_map = daily_strategies.to_dict()
    g = g.assign(strategy_type=[strat_map.get(d, []) for d in g.index.get_level_values('date')])
    return g


from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tools.sm_exceptions import ValueWarning
import warnings

def evaluate_arima_model(series, arima_order):
    """Fit ARIMA model and return AIC for evaluation."""
    try:
        model = ARIMA(series, order=arima_order)
        model_fit = model.fit()
        return model_fit.aic
    except Exception:
        return float("inf")

def arima_forecast_with_grid_search(accident_series: pd.Series, start_date: pd.Timestamp,
                                    horizon: int = 30, p_values: list = range(0, 6),
                                    d_values: list = range(0, 2), q_values: list = range(0, 6)) -> pd.DataFrame:
    # Pre-process series
    series = accident_series.asfreq('D').fillna(0)
    start_date = pd.to_datetime(start_date)
    
    # Suppress warnings
    warnings.filterwarnings("ignore", category=ValueWarning)

    # Define the hyperparameters to search through
    best_score, best_cfg = float("inf"), None
    
    for p in p_values:
        for d in d_values:
            for q in q_values:
                order = (p, d, q)
                try:
                    aic = evaluate_arima_model(series, order)
                    if aic < best_score:
                        best_score, best_cfg = aic, order
                except Exception as e:
                    continue
    
    # Fit the model with the best found order
    print(best_cfg)
    model = ARIMA(series, order=best_cfg)
    fit = model.fit()
    
    # Forecasting
    forecast_index = pd.date_range(start=start_date, periods=horizon, freq='D')
    res = fit.get_forecast(steps=horizon)
    df = res.summary_frame()
    df.index = forecast_index
    df.index.name = 'date'
    df.rename(columns={'mean': 'forecast'}, inplace=True)
    
    return df

# Example usage:
# dataframe = your_data_frame_here
# forecast_df = arima_forecast_with_grid_search(dataframe['accident_count'], start_date=pd.Timestamp('YYYY-MM-DD'), horizon=30)


def knn_forecast_counterfactual(accident_series: pd.Series,
                                intervention_date: pd.Timestamp,
                                lookback: int = 14,
                                horizon: int = 30):
    series = accident_series.asfreq('D').fillna(0)
    intervention_date = pd.to_datetime(intervention_date).normalize()

    df = pd.DataFrame({'y': series})
    for i in range(1, lookback + 1):
        df[f'lag_{i}'] = df['y'].shift(i)

    train = df.loc[:intervention_date - pd.Timedelta(days=1)].dropna()
    if len(train) < 5:
        return None, None
    X_train = train.filter(like='lag_').values
    y_train = train['y'].values
    knn = KNeighborsRegressor(n_neighbors=5)
    knn.fit(X_train, y_train)

    history = df.loc[:intervention_date - pd.Timedelta(days=1), 'y'].tolist()
    preds = []
    for _ in range(horizon):
        if len(history) < lookback:
            return None, None
        x = np.array(history[-lookback:][::-1]).reshape(1, -1)
        pred = knn.predict(x)[0]
        preds.append(pred)
        history.append(pred)

    pred_index = pd.date_range(intervention_date, periods=horizon, freq='D')
    return pd.Series(preds, index=pred_index, name='knn_pred'), None


def detect_anomalies(series: pd.Series, contamination: float = 0.1):
    series = series.asfreq('D').fillna(0)
    iso = IsolationForest(contamination=contamination, random_state=42)
    yhat = iso.fit_predict(series.values.reshape(-1, 1))
    anomaly_mask = (yhat == -1)
    anomaly_indices = series.index[anomaly_mask]

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=series.index, y=series.values, mode='lines', name='Accident Count'))
    fig.add_trace(go.Scatter(x=anomaly_indices, y=series.loc[anomaly_indices], mode='markers',
                             marker=dict(color='red', size=10), name='Anomalies'))
    fig.update_layout(title="Anomaly Detection in Accident Count",
                      xaxis_title="Date", yaxis_title="Count")
    return anomaly_indices, fig




def intervention_model(series: pd.Series,
                       intervention_date: pd.Timestamp,
                       intervention_type: str = 'persistent',
                       effect_type: str = 'sudden',
                       omega: float = 0.5,
                       decay: float = 10.0,
                       lag: int = 0):
    series = series.asfreq('D').fillna(0)
    intervention_date = pd.to_datetime(intervention_date)
    Z_t = pd.Series(0.0, index=series.index)
    if intervention_type == 'persistent':
        Z_t.loc[intervention_date:] = 1.0
    else:
        post_len = len(Z_t.loc[intervention_date:])
        Z_t.loc[intervention_date:] = np.exp(-np.arange(post_len) / decay)
    if effect_type == 'gradual':
        Z_t = Z_t * np.linspace(0, 1, len(Z_t))
    Z_t = Z_t.shift(lag).fillna(0)
    Y_t = series + omega * Z_t
    return Y_t, Z_t


def fit_and_extrapolate(series: pd.Series,
                        intervention_date: pd.Timestamp,
                        days: int = 30):

    series = series.asfreq('D').fillna(0)
    # ç»Ÿä¸€ä¸ºæ— æ—¶åŒºã€æŒ‰å¤©çš„æ—¶é—´æˆ³
    series.index = pd.to_datetime(series.index).tz_localize(None).normalize()
    intervention_date = pd.to_datetime(intervention_date).tz_localize(None).normalize()

    pre = series.loc[:intervention_date - pd.Timedelta(days=1)]
    if len(pre) < 5:
        return None, None, None

    x_pre = np.arange(len(pre))
    x_future = np.arange(len(pre), len(pre) + days)

    # 1ï¸âƒ£ GLMï¼šåŠ å…¥äºŒæ¬¡é¡¹
    X_pre_glm = sm.add_constant(np.column_stack([x_pre, x_pre**2]))
    glm = sm.GLM(pre.values, X_pre_glm, family=sm.families.Poisson())
    glm_res = glm.fit()
    X_future_glm = sm.add_constant(np.column_stack([x_future, x_future**2]))
    glm_pred = glm_res.predict(X_future_glm)

    # SVR
    # 2ï¸âƒ£ SVRï¼šåŠ æ ‡å‡†åŒ– & è°ƒå‚ / æ”¹çº¿æ€§æ ¸
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import make_pipeline

    svr = make_pipeline(
        StandardScaler(),
        SVR(kernel='rbf', C=10, gamma=0.1)   # æˆ– kernel='linear'
    )
    svr.fit(x_pre.reshape(-1, 1), pre.values)
    svr_pred = svr.predict(x_future.reshape(-1, 1))

    # ç›®æ ‡é¢„æµ‹ç´¢å¼•ï¼ˆæœªæ¥å¯èƒ½è¶…å‡ºå†å²èŒƒå›´ â€”â€” ç”¨ reindexï¼Œä¸è¦ .loc[...]ï¼‰
    post_index = pd.date_range(intervention_date, periods=days, freq='D')

    glm_pred = pd.Series(glm_pred, index=post_index, name='glm_pred')
    svr_pred = pd.Series(svr_pred, index=post_index, name='svr_pred')

    # âœ… å…³é”®ä¿®å¤ï¼šå¯¹ä¸å­˜åœ¨çš„æ—¥æœŸè¡¥ NaNï¼Œè€Œä¸æ˜¯ .loc[post_index]
    post = series.reindex(post_index)

    residuals = pd.Series(post.values - svr_pred[:len(post)],
                          index=post_index, name='residual')

    return glm_pred, svr_pred, residuals


def evaluate_strategy_effectiveness(actual_series: pd.Series,
                                    counterfactual_series: pd.Series,
                                    severity_series: pd.Series,
                                    strategy_date: pd.Timestamp,
                                    window: int = 30):
    strategy_date = pd.to_datetime(strategy_date)
    pre_sev = severity_series.loc[strategy_date - pd.Timedelta(days=window):strategy_date - pd.Timedelta(days=1)].sum()
    post_sev = severity_series.loc[strategy_date:strategy_date + pd.Timedelta(days=window - 1)].sum()
    actual_post = actual_series.loc[strategy_date:strategy_date + pd.Timedelta(days=window - 1)]
    counter_post = counterfactual_series.loc[strategy_date:strategy_date + pd.Timedelta(days=window - 1)]
    counter_post = counter_post.reindex(actual_post.index)
    effective_days = (actual_post < counter_post).sum()
    count_effective = effective_days >= (window / 2)
    severity_effective = post_sev < pre_sev
    cf_sum = counter_post.sum()
    F1 = (cf_sum - actual_post.sum()) / cf_sum if cf_sum > 0 else 0.0
    F2 = (pre_sev - post_sev) / pre_sev if pre_sev > 0 else 0.0
    if F1 > 0.5 and F2 > 0.5:
        safety_state = 'ä¸€çº§'
    elif F1 > 0.3:
        safety_state = 'äºŒçº§'
    else:
        safety_state = 'ä¸‰çº§'
    return count_effective, severity_effective, (F1, F2), safety_state


def generate_output_and_recommendations(combined_data: pd.DataFrame,
                                        strategy_types: list,
                                        region: str = 'å…¨å¸‚',
                                        horizon: int = 30):
    results = {}
    accident_series = combined_data['accident_count']
    severity_series = combined_data['severity']
    for strategy in strategy_types:
        has_strategy = combined_data['strategy_type'].apply(lambda x: strategy in x)
        if not has_strategy.any():
            continue
        intervention_date = has_strategy[has_strategy].index[0]
        glm_pred, svr_pred, residuals = fit_and_extrapolate(accident_series, intervention_date, days=horizon)
        if svr_pred is None:
            continue
        count_eff, sev_eff, (F1, F2), state = evaluate_strategy_effectiveness(
            actual_series=accident_series,
            counterfactual_series=svr_pred,
            severity_series=severity_series,
            strategy_date=intervention_date,
            window=horizon
        )
        results[strategy] = {
            'effect_strength': float(residuals.mean()),
            'adaptability': float(F1 + F2),
            'count_effective': bool(count_eff),
            'severity_effective': bool(sev_eff),
            'safety_state': state,
            'F1': float(F1),
            'F2': float(F2),
            'intervention_date': str(intervention_date.date())
        }
    best_strategy = max(results, key=lambda x: results[x]['adaptability']) if results else None
    recommendation = f"å»ºè®®åœ¨{region}åŒºåŸŸé•¿æœŸå®æ–½ç­–ç•¥ç±»å‹ {best_strategy}" if best_strategy else "æ— è¶³å¤Ÿæ•°æ®æ¨èç­–ç•¥"
    pd.DataFrame(results).T.to_csv('strategy_evaluation_results.csv', encoding='utf-8-sig')
    with open('recommendation.txt', 'w', encoding='utf-8') as f:
        f.write(recommendation)
    return results, recommendation


# =======================
# 3. UI Helpers
# =======================
def hash_like(obj: str) -> str:
    return hashlib.md5(obj.encode('utf-8')).hexdigest()[:8]


def compute_kpis(df_city: pd.DataFrame, arima_df: pd.DataFrame | None,
                 today: pd.Timestamp, window:int=30):
    # ä»Šæ—¥/æ˜¨æ—¥
    today_date = pd.to_datetime(today.date())
    yesterday = today_date - pd.Timedelta(days=1)
    this_week_start = today_date - pd.Timedelta(days=today_date.weekday())  # å‘¨ä¸€
    last_week_start = this_week_start - pd.Timedelta(days=7)
    this_week_end = today_date

    today_cnt = int(df_city['accident_count'].get(today_date, 0))
    yest_cnt = int(df_city['accident_count'].get(yesterday, 0))
    wow = (today_cnt - yest_cnt) / yest_cnt if yest_cnt > 0 else 0.0

    this_week = df_city.loc[this_week_start:this_week_end]['accident_count'].sum()
    last_week = df_city.loc[last_week_start:last_week_start + pd.Timedelta(days=(this_week_end - this_week_start).days)]['accident_count'].sum()
    yoy = (this_week - last_week) / last_week if last_week > 0 else 0.0

    # é¢„æµ‹åå·®ï¼ˆè¿‘7å¤©ï¼‰
    forecast_bias = None
    if arima_df is not None:
        recent = df_city.index.max() - pd.Timedelta(days=6)
        actual = df_city.loc[recent:df_city.index.max(), 'accident_count']
        fcst = arima_df['forecast'].reindex(actual.index).fillna(method='ffill')
        denom = fcst.replace(0, np.nan)
        bias = (np.abs(actual - fcst) / denom).dropna()
        forecast_bias = float(bias.mean()) if len(bias) else None

    # ç­–ç•¥è¦†ç›–ï¼ˆè¿‘30å¤©ï¼‰
    last_window = df_city.index.max() - pd.Timedelta(days=window-1)
    strat_days = df_city.loc[last_window:, 'strategy_type'].apply(lambda x: len(x) > 0).sum()
    coverage = strat_days / window

    # ä¸Šçº¿ç­–ç•¥æ•°ï¼ˆå»é‡ï¼‰
    active_strats = set(s for lst in df_city.loc[last_window:, 'strategy_type'] for s in lst)
    active_count = len(active_strats)

    # è¿‘30å¤©å®‰å…¨ç­‰çº§ï¼ˆç”¨ generate_output_and_recommendations é‡Œ best çš„ç­‰çº§ï¼‰
    # è¿™é‡Œåªå–æœ€è¿‘å‡ºç°è¿‡çš„ç­–ç•¥åšè¯„ä¼°
    strategies = sorted(active_strats)
    safety_state = 'â€”'
    if strategies:
        res, _ = generate_output_and_recommendations(df_city.loc[last_window:], strategies, region='å…¨å¸‚', horizon=min(30, len(df_city.loc[last_window:])))
        if res:
            # å–é€‚é…åº¦æœ€é«˜çš„ç­–ç•¥çš„å®‰å…¨ç­‰çº§
            best = max(res, key=lambda k: res[k]['adaptability'])
            safety_state = res[best]['safety_state']

    return {
        'today_cnt': today_cnt,
        'wow': wow,
        'this_week': int(this_week),
        'yoy': yoy,
        'forecast_bias': forecast_bias,
        'active_count': active_count,
        'coverage': coverage,
        'safety_state': safety_state
    }


def significance_test(pre: pd.Series, post: pd.Series):
    pre = pre.dropna(); post = post.dropna()
    if len(pre) < 3 or len(post) < 3:
        return None, None
    if HAS_SCIPY:
        try:
            stat, p = ttest_ind(pre, post, equal_var=False)
        except Exception:
            stat, p = mannwhitneyu(pre, post, alternative='two-sided')
        return float(stat), float(p)
    return None, None


def save_fig_as_html(fig, filename):
    html = fig.to_html(full_html=True, include_plotlyjs='cdn')
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(html)
    return filename

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error
from statsmodels.tsa.arima.model import ARIMA

# ä¾èµ–ï¼šå·²åœ¨è„šæœ¬å‰é¢å®šä¹‰çš„  knn_forecast_counterfactual()  å’Œ  fit_and_extrapolate()
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error
from statsmodels.tsa.arima.model import ARIMA

# ä¾èµ–ï¼šknn_forecast_counterfactualã€fit_and_extrapolate å·²å­˜åœ¨

def evaluate_models(series: pd.Series,
                    horizon: int = 30,
                    lookback: int = 14,
                    p_values: range = range(0, 6),
                    d_values: range = range(0, 2),
                    q_values: range = range(0, 6)) -> pd.DataFrame:
    """
    ç•™å‡ºæ³•ï¼ˆæœ€å horizon å¤©ä½œä¸ºéªŒè¯é›†ï¼‰æ¯”è¾ƒ ARIMA / KNN / GLM / SVRï¼Œ
    è¾“å‡º MAEãƒ»RMSEãƒ»MAPEï¼Œå¹¶æŒ‰ RMSE å‡åºæ’åºã€‚
    """
    # ç»Ÿä¸€æ—¥é¢‘ & ç¼ºå¤±è¡¥é›¶
    series = series.asfreq('D').fillna(0)
    if len(series) <= horizon + 10:
        raise ValueError("åºåˆ—å¤ªçŸ­ï¼Œæ— æ³•ç•™å‡º %d å¤©è¿›è¡Œè¯„ä¼°ã€‚" % horizon)

    train, test = series.iloc[:-horizon], series.iloc[-horizon:]

    def _to_series_like(pred, a_index):
        # å°†ä»»æ„é¢„æµ‹å¯¹é½æˆä¸ actual åŒç´¢å¼•çš„ Series
        if isinstance(pred, pd.Series):
            return pred.reindex(a_index)
        return pd.Series(pred, index=a_index)

    def _metrics(a: pd.Series, p) -> dict:
        p = _to_series_like(p, a.index).astype(float)
        a = a.astype(float)

        mae = mean_absolute_error(a, p)

        # å…¼å®¹æ—§ç‰ˆ sklearnï¼šæ²¡æœ‰ squared å‚æ•°æ—¶æ‰‹åŠ¨å¼€æ–¹
        try:
            rmse = mean_squared_error(a, p, squared=False)
        except TypeError:
            rmse = mean_squared_error(a, p) ** 0.5

        # å¿½ç•¥åˆ†æ¯ä¸º 0 çš„æ ·æœ¬
        mape = np.nanmean(np.abs((a - p) / np.where(a == 0, np.nan, a))) * 100
        return {"MAE": mae, "RMSE": rmse, "MAPE": mape}

    results = {}

    # ---------- ARIMA ----------
    best_aic, best_order = float('inf'), None
    for p in p_values:
        for d in d_values:
            for q in q_values:
                try:
                    aic = ARIMA(train, order=(p, d, q)).fit().aic
                    if aic < best_aic:
                        best_aic, best_order = aic, (p, d, q)
                except Exception:
                    continue
    arima_train = train.asfreq('D').fillna(0)
    arima_pred = ARIMA(arima_train, order=best_order).fit().forecast(steps=horizon)
    results['ARIMA'] = _metrics(test, arima_pred)

    # ---------- KNN ----------
    try:
        knn_pred, _ = knn_forecast_counterfactual(series,
                                                  train.index[-1] + pd.Timedelta(days=1),
                                                  lookback=lookback,
                                                  horizon=horizon)
        if knn_pred is not None:
            results['KNN'] = _metrics(test, knn_pred)
    except Exception:
        pass

    # ---------- GLM & SVR ----------
    try:
        glm_pred, svr_pred, _ = fit_and_extrapolate(series,
                                                    train.index[-1] + pd.Timedelta(days=1),
                                                    days=horizon)
        if glm_pred is not None:
            results['GLM'] = _metrics(test, glm_pred)
        if svr_pred is not None:
            results['SVR'] = _metrics(test, svr_pred)
    except Exception:
        pass

    return (pd.DataFrame(results)
            .T.sort_values('RMSE')
            .round(3))


import re
from collections import Counter
import jieba

def parse_and_standardize_locations(accident_data):
    """è§£æå’Œæ ‡å‡†åŒ–äº‹æ•…åœ°ç‚¹"""
    df = accident_data.copy()
    
    # æå–å…³é”®è·¯æ®µä¿¡æ¯
    def extract_road_info(location):
        if pd.isna(location):
            return "æœªçŸ¥è·¯æ®µ"
        
        location = str(location)
        
        # å¸¸è§è·¯æ®µå…³é”®è¯
        road_keywords = ['è·¯', 'é“', 'è¡—', 'å··', 'è·¯å£', 'äº¤å‰å£', 'å¤§é“', 'å…¬è·¯']
        area_keywords = ['æ–°åŸ', 'ä¸´åŸ', 'åƒå²›', 'ç¿å±±', 'æµ·å¤©', 'æµ·å®‡', 'å®šæ²ˆ', 'æ»¨æµ·', 'æ¸¯å²›', 'ä½“è‚²', 'é•¿å‡', 'é‡‘å²›', 'æ¡ƒæ¹¾']
        
        # æå–åŒ…å«å…³é”®è¯çš„è·¯æ®µ
        for keyword in road_keywords + area_keywords:
            if keyword in location:
                # æå–ä»¥è¯¥å…³é”®è¯ä¸ºä¸­å¿ƒçš„è·¯æ®µåç§°
                pattern = f'[^ï¼Œã€‚]*{keyword}[^ï¼Œã€‚]*'
                matches = re.findall(pattern, location)
                if matches:
                    return matches[0].strip()
        
        return location

    df['standardized_location'] = df['äº‹æ•…å…·ä½“åœ°ç‚¹'].apply(extract_road_info)
    
    # è¿›ä¸€æ­¥æ¸…ç†å’Œæ ‡å‡†åŒ–
    location_mapping = {
        'æ–°åŸåƒå²›è·¯': 'åƒå²›è·¯',
        'åƒå²›è·¯æµ·å¤©å¤§é“': 'åƒå²›è·¯æµ·å¤©å¤§é“å£',
        'æµ·å¤©å¤§é“åƒå²›è·¯': 'åƒå²›è·¯æµ·å¤©å¤§é“å£',
        'æ–°åŸç¿å±±è·¯': 'ç¿å±±è·¯',
        'ç¿å±±è·¯é‡‘å²›è·¯': 'ç¿å±±è·¯é‡‘å²›è·¯å£',
        # æ·»åŠ æ›´å¤šæ ‡å‡†åŒ–æ˜ å°„...
    }
    
    df['standardized_location'] = df['standardized_location'].replace(location_mapping)
    
    return df

def analyze_location_frequency(accident_data, time_window='7D'):
    """åˆ†æåœ°ç‚¹äº‹æ•…é¢‘æ¬¡"""
    df = parse_and_standardize_locations(accident_data)
    
    # è®¡ç®—æ—¶é—´çª—å£
    recent_cutoff = df['äº‹æ•…æ—¶é—´'].max() - pd.Timedelta(time_window)
    
    # æ€»ä½“ç»Ÿè®¡
    overall_stats = df.groupby('standardized_location').agg({
        'äº‹æ•…æ—¶é—´': ['count', 'max'],  # äº‹æ•…æ€»æ•°å’Œæœ€è¿‘æ—¶é—´
        'äº‹æ•…ç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'è´¢æŸ',
        'é“è·¯ç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'åŸåŒºé“è·¯',
        'è·¯å£è·¯æ®µç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'æ™®é€šè·¯æ®µ'
    })
    
    # æ‰å¹³åŒ–åˆ—å
    overall_stats.columns = ['accident_count', 'last_accident', 'main_accident_type', 'main_road_type', 'main_intersection_type']
    
    # è¿‘æœŸç»Ÿè®¡
    recent_accidents = df[df['äº‹æ•…æ—¶é—´'] >= recent_cutoff]
    recent_stats = recent_accidents.groupby('standardized_location').agg({
        'äº‹æ•…æ—¶é—´': 'count',
        'äº‹æ•…ç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'è´¢æŸ'
    }).rename(columns={'äº‹æ•…æ—¶é—´': 'recent_count', 'äº‹æ•…ç±»å‹': 'recent_accident_type'})
    
    # åˆå¹¶æ•°æ®
    result = overall_stats.merge(recent_stats, left_index=True, right_index=True, how='left').fillna(0)
    result['recent_count'] = result['recent_count'].astype(int)
    
    # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
    result['trend_ratio'] = result['recent_count'] / result['accident_count']
    result['days_since_last'] = (df['äº‹æ•…æ—¶é—´'].max() - result['last_accident']).dt.days
    
    return result.sort_values(['recent_count', 'accident_count'], ascending=False)


def generate_intelligent_strategies(hotspot_df, time_period='æœ¬å‘¨'):
    """ç”Ÿæˆæ™ºèƒ½é’ˆå¯¹æ€§ç­–ç•¥"""
    strategies = []
    
    for location_name, location_data in hotspot_df.iterrows():
        accident_count = location_data['accident_count']
        recent_count = location_data['recent_count']
        accident_type = location_data['main_accident_type']
        road_type = location_data['main_road_type']
        intersection_type = location_data['main_intersection_type']
        trend_ratio = location_data['trend_ratio']
        
        # åŸºç¡€ä¿¡æ¯
        base_info = f"{time_period}å¯¹ã€{location_name}ã€‘"
        data_support = f"ï¼ˆè¿‘æœŸ{int(recent_count)}èµ·ï¼Œç´¯è®¡{int(accident_count)}èµ·ï¼Œ{accident_type}ä¸ºä¸»ï¼‰"
        
        # æ™ºèƒ½ç­–ç•¥ç”Ÿæˆ
        strategy_parts = []
        
        # åŸºäºäº‹æ•…ç±»å‹
        if accident_type == 'è´¢æŸ':
            strategy_parts.append("åŠ å¼ºè¿æ³•æŸ¥å¤„")
            if 'ä¿¡å·ç¯' in intersection_type:
                strategy_parts.append("æ•´æ²»é—¯çº¢ç¯ã€ä¸æŒ‰è§„å®šè®©è¡Œ")
            else:
                strategy_parts.append("æ•´æ²»è¿æ³•å˜é“ã€è¶…é€Ÿè¡Œé©¶")
        elif accident_type == 'ä¼¤äºº':
            strategy_parts.append("ä¼˜åŒ–äº¤é€šç»„ç»‡")
            strategy_parts.append("å¢è®¾å®‰å…¨è®¾æ–½")
            if recent_count >= 2:
                strategy_parts.append("å¼€å±•ä¸“é¡¹æ•´æ²»")
        
        # åŸºäºè·¯å£ç±»å‹
        if intersection_type == 'ä¿¡å·ç¯è·¯å£':
            strategy_parts.append("ä¼˜åŒ–ä¿¡å·é…æ—¶")
        elif intersection_type == 'éä¿¡å·ç¯è·¯å£':
            strategy_parts.append("å®Œå–„è®©è¡Œæ ‡å¿—")
        elif intersection_type == 'æ™®é€šè·¯æ®µ':
            if trend_ratio > 0.3:  # è¿‘æœŸäº‹æ•…å æ¯”é«˜
                strategy_parts.append("åŠ å¼ºå·¡é€»ç®¡æ§")
        
        # åŸºäºè¶‹åŠ¿
        if trend_ratio > 0.5:
            strategy_parts.append("åˆ—ä¸ºé‡ç‚¹ç®¡æ§è·¯æ®µ")
        if location_data['days_since_last'] <= 3:
            strategy_parts.append("è¿‘æœŸéœ€é‡ç‚¹å…³æ³¨")
        
        # ç»„åˆç­–ç•¥
        if strategy_parts:
            strategy = base_info + "ï¼Œ" + "ï¼Œ".join(strategy_parts) + data_support
        else:
            strategy = base_info + "åˆ†æäº‹æ•…æˆå› ï¼Œåˆ¶å®šç»¼åˆæ•´æ²»æ–¹æ¡ˆ" + data_support
        
        strategies.append(strategy)
    
    return strategies

def calculate_location_risk_score(hotspot_df):
    """è®¡ç®—è·¯å£é£é™©è¯„åˆ†"""
    df = hotspot_df.copy()
    
    # äº‹æ•…é¢‘æ¬¡å¾—åˆ† (0-40åˆ†)
    df['frequency_score'] = (df['accident_count'] / df['accident_count'].max() * 40).clip(0, 40)
    
    # è¿‘æœŸè¶‹åŠ¿å¾—åˆ† (0-30åˆ†)
    df['trend_score'] = (df['trend_ratio'] * 30).clip(0, 30)
    
    # äº‹æ•…ä¸¥é‡åº¦å¾—åˆ† (0-20åˆ†)
    severity_map = {'è´¢æŸ': 5, 'ä¼¤äºº': 15, 'äº¡äºº': 20}
    df['severity_score'] = df['main_accident_type'].map(severity_map).fillna(5)
    
    # æ—¶é—´ç´§è¿«åº¦å¾—åˆ† (0-10åˆ†)
    df['urgency_score'] = ((30 - df['days_since_last']) / 30 * 10).clip(0, 10)
    
    # æ€»åˆ†
    df['risk_score'] = df['frequency_score'] + df['trend_score'] + df['severity_score'] + df['urgency_score']
    
    # é£é™©ç­‰çº§
    conditions = [
        df['risk_score'] >= 70,
        df['risk_score'] >= 50,
        df['risk_score'] >= 30
    ]
    choices = ['é«˜é£é™©', 'ä¸­é£é™©', 'ä½é£é™©']
    df['risk_level'] = np.select(conditions, choices, default='ä¸€èˆ¬é£é™©')
    
    return df.sort_values('risk_score', ascending=False)


# =======================
# 4. App
# =======================
def run_streamlit_app():
    st.set_page_config(page_title="Traffic Safety Analysis", layout="wide")
    st.title("ğŸš¦ Traffic Safety Intervention Analysis System")

    # Sidebar â€” Upload & Global Filters & Auto Refresh
    st.sidebar.header("æ•°æ®ä¸ç­›é€‰")
    
    # Create a form for data inputs to batch updates
    with st.sidebar.form(key="data_input_form"):
        accident_file = st.file_uploader("ä¸Šä¼ äº‹æ•…æ•°æ® (Excel)", type=['xlsx'])
        strategy_file = st.file_uploader("ä¸Šä¼ äº¤é€šç­–ç•¥æ•°æ® (Excel)", type=['xlsx'])

        # Global filters
        st.markdown("---")
        st.subheader("å…¨å±€ç­›é€‰å™¨")
        # Placeholder for region selection (will be populated after data is loaded)
        region_sel = st.selectbox("åŒºåŸŸ", options=["å…¨å¸‚"], index=0, key="region_select")
        # Default date range (will be updated after data is loaded)
        min_date = pd.to_datetime('2022-01-01').date()
        max_date = pd.to_datetime('2022-12-31').date()
        date_range = st.date_input("æ—¶é—´èŒƒå›´", value=(min_date, max_date), min_value=min_date, max_value=max_date)
        strat_filter = st.multiselect("ç­–ç•¥ç±»å‹ï¼ˆè¿‡æ»¤ï¼‰", options=[], help="ä¸ºç©ºè¡¨ç¤ºä¸è¿‡æ»¤ç­–ç•¥ï¼›é€‰æ‹©åä»…ä¿ç•™å½“å¤©åŒ…å«æ‰€é€‰ç­–ç•¥çš„æ—¥æœŸ")
        
        # Apply button for data loading and filtering
        apply_button = st.form_submit_button("åº”ç”¨æ•°æ®ä¸ç­›é€‰")

    # Auto-refresh controls (outside the form, as itâ€™s independent)
    st.sidebar.markdown("---")
    st.sidebar.subheader("å®æ—¶åˆ·æ–°")
    auto = st.sidebar.checkbox("è‡ªåŠ¨åˆ·æ–°", value=False, help="å¯ç”¨åå°†æŒ‰é—´éš”è‡ªåŠ¨åˆ·æ–°é¡µé¢")
    interval = st.sidebar.number_input("åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰", min_value=5, max_value=600, value=30, step=5)
    if auto and HAS_AUTOREFRESH:
        st_autorefresh(interval=int(interval*1000), key="autorefresh")
    elif auto and not HAS_AUTOREFRESH:
        st.sidebar.info("æœªå®‰è£… `streamlit-autorefresh`ï¼Œè¯·ä½¿ç”¨ä¸Šæ–¹â€œé‡æ–°è¿è¡Œâ€æŒ‰é’®æˆ–å…³é—­å†å¼€å¯æ­¤å¼€å…³ã€‚")

    # Add OpenAI API key input in sidebar
    st.sidebar.markdown("---")
    st.sidebar.subheader("GPT API é…ç½®")
    openai_api_key = st.sidebar.text_input("GPT API Key", value='sk-dQhKOOG48iVEfgJfAb14458dA4474fB09aBbE8153d4aB3Fc', type="password", help="ç”¨äºGPTåˆ†æç»“æœçš„APIå¯†é’¥")
    open_ai_base_url = st.sidebar.text_input("GPT Base Url", value='https://az.gptplus5.com/v1', type='default')

    # Initialize session state to store processed data
    if 'processed_data' not in st.session_state:
        st.session_state['processed_data'] = {
            'combined_city': None,
            'combined_by_region': None,
            'accident_data': None,
            'strategy_data': None,
            'all_regions': ["å…¨å¸‚"],
            'all_strategy_types': [],
            'min_date': min_date,
            'max_date': max_date,
            'region_sel': "å…¨å¸‚",
            'date_range': (min_date, max_date),
            'strat_filter': []
        }

    # Process data only when Apply button is clicked
    if apply_button and accident_file and strategy_file:
        with st.spinner("æ•°æ®è½½å…¥ä¸­â€¦"):
            # Load and clean data
            accident_data, strategy_data = load_and_clean_data(accident_file, strategy_file)
            combined_city = aggregate_daily_data(accident_data, strategy_data)
            combined_by_region = aggregate_daily_data_by_region(accident_data, strategy_data)

            # Update available options for filters
            all_regions = ["å…¨å¸‚"] + sorted(accident_data['region'].unique().tolist())
            all_strategy_types = sorted({s for lst in combined_city['strategy_type'] for s in lst})
            min_date = combined_city.index.min().date()
            max_date = combined_city.index.max().date()

            # Store processed data in session state
            st.session_state['processed_data'].update({
                'combined_city': combined_city,
                'combined_by_region': combined_by_region,
                'accident_data': accident_data,
                'strategy_data': strategy_data,
                'all_regions': all_regions,
                'all_strategy_types': all_strategy_types,
                'min_date': min_date,
                'max_date': max_date,
                'region_sel': region_sel,
                'date_range': date_range,
                'strat_filter': strat_filter
            })

    # Retrieve data from session state
    combined_city = st.session_state['processed_data']['combined_city']
    combined_by_region = st.session_state['processed_data']['combined_by_region']
    accident_data = st.session_state['processed_data']['accident_data']
    strategy_data = st.session_state['processed_data']['strategy_data']
    all_regions = st.session_state['processed_data']['all_regions']
    all_strategy_types = st.session_state['processed_data']['all_strategy_types']
    min_date = st.session_state['processed_data']['min_date']
    max_date = st.session_state['processed_data']['max_date']
    region_sel = st.session_state['processed_data']['region_sel']
    date_range = st.session_state['processed_data']['date_range']
    strat_filter = st.session_state['processed_data']['strat_filter']

    # Update selectbox and multiselect options dynamically (outside the form for display)
    st.sidebar.markdown("---")
    st.sidebar.subheader("å½“å‰ç­›é€‰çŠ¶æ€")
    st.sidebar.write(f"åŒºåŸŸ: {region_sel}")
    st.sidebar.write(f"æ—¶é—´èŒƒå›´: {date_range[0]} è‡³ {date_range[1]}")
    st.sidebar.write(f"ç­–ç•¥ç±»å‹: {', '.join(strat_filter) or 'æ— '}")

    # Proceed only if data is available
    if combined_city is not None and combined_by_region is not None:
        start_dt = pd.to_datetime(date_range[0])
        end_dt = pd.to_datetime(date_range[1])
        if region_sel == "å…¨å¸‚":
            base = combined_city.loc[start_dt:end_dt].copy()
        else:
            block = combined_by_region.xs(region_sel, level='region').copy()
            base = block.loc[start_dt:end_dt]
        if strat_filter:
            mask = base['strategy_type'].apply(lambda x: any(s in x for s in strat_filter))
            base = base[mask]

        # Last refresh info
        if 'last_refresh' not in st.session_state:
            st.session_state['last_refresh'] = datetime.now()
        last_refresh = st.session_state['last_refresh']

        # Compute ARIMA for KPI bias
        arima_df = None
        try:
            arima_df = arima_forecast_with_grid_search(
                base['accident_count'], base.index.max() + pd.Timedelta(days=1), horizon=7
            )
        except Exception:
            pass

        # KPI Overview
        kpi = compute_kpis(base, arima_df, today=pd.Timestamp('2022-12-01'))
        c1, c2, c3, c4, c5, c6 = st.columns(6)
        c1.metric("ä»Šæ—¥äº‹æ•…æ•°", f"{kpi['today_cnt']}", f"{kpi['wow']*100:.1f}% ç¯æ¯”")
        c2.metric("æœ¬å‘¨äº‹æ•…æ•°", f"{kpi['this_week']}", f"{kpi['yoy']*100:.1f}% åŒæ¯”")
        c3.metric("è¿‘7å¤©é¢„æµ‹åå·®", ("{:.1f}%".format(kpi['forecast_bias']*100) if kpi['forecast_bias'] is not None else "â€”"))
        c4.metric("è¿‘30å¤©ç­–ç•¥æ•°", f"{kpi['active_count']}")
        c5.metric("è¿‘30å¤©ç­–ç•¥è¦†ç›–ç‡", f"{kpi['coverage']*100:.1f}%")
        c6.metric("è¿‘30å¤©å®‰å…¨ç­‰çº§", kpi['safety_state'])

        # Top-right meta
        meta_col1, meta_col2 = st.columns([4, 1])
        with meta_col2:
            st.caption(f"ğŸ•’ æœ€è¿‘åˆ·æ–°ï¼š{last_refresh.strftime('%Y-%m-%d %H:%M:%S')}")

        # Tabs (add new tab for GPT analysis)
        tab_dash, tab_pred, tab_eval, tab_anom, tab_strat, tab_comp, tab_sim, tab_gpt, tab_hotspot = st.tabs(
            ["ğŸ  æ€»è§ˆ", "ğŸ“ˆ é¢„æµ‹æ¨¡å‹", "ğŸ“Š æ¨¡å‹è¯„ä¼°", "âš ï¸ å¼‚å¸¸æ£€æµ‹", "ğŸ“ ç­–ç•¥è¯„ä¼°", "âš–ï¸ ç­–ç•¥å¯¹æ¯”", "ğŸ§ª æƒ…æ™¯æ¨¡æ‹Ÿ", "ğŸ” GPT åˆ†æ", "ğŸ“ äº‹æ•…çƒ­ç‚¹"]
        )


        with tab_hotspot:
            st.header("ğŸ“ äº‹æ•…å¤šå‘è·¯å£åˆ†æ")
            st.markdown("ç‹¬ç«‹åˆ†æäº‹æ•…æ•°æ®ï¼Œè¯†åˆ«é«˜é£é™©è·¯å£å¹¶ç”Ÿæˆé’ˆå¯¹æ€§ç­–ç•¥")
            
            # ç‹¬ç«‹æ–‡ä»¶ä¸Šä¼ 
            st.subheader("ğŸ“ æ•°æ®ä¸Šä¼ ")
            hotspot_file = st.file_uploader("ä¸Šä¼ äº‹æ•…æ•°æ®æ–‡ä»¶", type=['xlsx'], key="hotspot_uploader")
            
            if hotspot_file is not None:
                try:
                    # åŠ è½½æ•°æ®
                    @st.cache_data(show_spinner=False)
                    def load_hotspot_data(uploaded_file):
                        """ç‹¬ç«‹åŠ è½½äº‹æ•…çƒ­ç‚¹åˆ†ææ•°æ®"""
                        df = pd.read_excel(uploaded_file, sheet_name=None)
                        accident_data = pd.concat(df.values(), ignore_index=True)
                        
                        # æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
                        accident_data['äº‹æ•…æ—¶é—´'] = pd.to_datetime(accident_data['äº‹æ•…æ—¶é—´'])
                        accident_data = accident_data.dropna(subset=['äº‹æ•…æ—¶é—´', 'æ‰€åœ¨è¡—é“', 'äº‹æ•…ç±»å‹', 'äº‹æ•…å…·ä½“åœ°ç‚¹'])
                        
                        # æ·»åŠ ä¸¥é‡åº¦è¯„åˆ†
                        severity_map = {'è´¢æŸ': 1, 'ä¼¤äºº': 2, 'äº¡äºº': 4}
                        accident_data['severity'] = accident_data['äº‹æ•…ç±»å‹'].map(severity_map).fillna(1)
                        
                        return accident_data
                    
                    with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
                        accident_data = load_hotspot_data(hotspot_file)
                    
                    # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
                    st.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼š{len(accident_data)} æ¡äº‹æ•…è®°å½•")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ•°æ®æ—¶é—´èŒƒå›´", 
                                f"{accident_data['äº‹æ•…æ—¶é—´'].min().strftime('%Y-%m-%d')} è‡³ {accident_data['äº‹æ•…æ—¶é—´'].max().strftime('%Y-%m-%d')}")
                    with col2:
                        st.metric("äº‹æ•…ç±»å‹åˆ†å¸ƒ", 
                                f"è´¢æŸ: {len(accident_data[accident_data['äº‹æ•…ç±»å‹']=='è´¢æŸ'])}èµ·")
                    with col3:
                        st.metric("æ¶‰åŠåŒºåŸŸ", 
                                f"{accident_data['æ‰€åœ¨è¡—é“'].nunique()}ä¸ªè¡—é“")
                    
                    # åœ°ç‚¹æ ‡å‡†åŒ–å‡½æ•°ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
                    def standardize_hotspot_locations(df):
                        """æ ‡å‡†åŒ–äº‹æ•…åœ°ç‚¹"""
                        df = df.copy()
                        
                        def extract_road_info(location):
                            if pd.isna(location):
                                return "æœªçŸ¥è·¯æ®µ"
                            
                            location = str(location)
                            
                            # å¸¸è§è·¯æ®µå…³é”®è¯
                            road_keywords = ['è·¯', 'é“', 'è¡—', 'å··', 'è·¯å£', 'äº¤å‰å£', 'å¤§é“', 'å…¬è·¯', 'å£']
                            area_keywords = ['æ–°åŸ', 'ä¸´åŸ', 'åƒå²›', 'ç¿å±±', 'æµ·å¤©', 'æµ·å®‡', 'å®šæ²ˆ', 'æ»¨æµ·', 'æ¸¯å²›', 'ä½“è‚²', 'é•¿å‡', 'é‡‘å²›', 'æ¡ƒæ¹¾']
                            
                            # æå–åŒ…å«å…³é”®è¯çš„è·¯æ®µ
                            for keyword in road_keywords + area_keywords:
                                if keyword in location:
                                    # ç®€åŒ–åœ°ç‚¹åç§°
                                    words = location.split()
                                    for word in words:
                                        if keyword in word:
                                            return word
                                    return location
                            
                            # å¦‚æœæ²¡æ‰¾åˆ°å…³é”®è¯ï¼Œè¿”å›åŸåœ°ç‚¹ï¼ˆæˆªæ–­è¿‡é•¿çš„ï¼‰
                            return location[:20] if len(location) > 20 else location
                        
                        df['standardized_location'] = df['äº‹æ•…å…·ä½“åœ°ç‚¹'].apply(extract_road_info)
                        
                        # æ‰‹åŠ¨æ ‡å‡†åŒ–æ˜ å°„ï¼ˆæ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼‰
                        location_mapping = {
                            'æ–°åŸåƒå²›è·¯': 'åƒå²›è·¯',
                            'åƒå²›è·¯æµ·å¤©å¤§é“': 'åƒå²›è·¯æµ·å¤©å¤§é“å£',
                            'æµ·å¤©å¤§é“åƒå²›è·¯': 'åƒå²›è·¯æµ·å¤©å¤§é“å£',
                            'æ–°åŸç¿å±±è·¯': 'ç¿å±±è·¯',
                            'ç¿å±±è·¯é‡‘å²›è·¯': 'ç¿å±±è·¯é‡‘å²›è·¯å£',
                            'æµ·å¤©å¤§é“ä¸´é•¿è·¯': 'æµ·å¤©å¤§é“ä¸´é•¿è·¯å£',
                            'å®šæ²ˆè·¯å«ç”ŸåŒ»é™¢é—¨å£': 'å®šæ²ˆè·¯åŒ»é™¢æ®µ',
                            'ç¿å±±è·¯æµ·åŸè·¯è¥¿å£': 'ç¿å±±è·¯æµ·åŸè·¯å£',
                            'æµ·å®‡é“è·¯å£': 'æµ·å®‡é“',
                            'æµ·å¤©å¤§é“è·¯å£': 'æµ·å¤©å¤§é“',
                            'å®šæ²ˆè·¯äº¤å‰è·¯å£': 'å®šæ²ˆè·¯',
                            'åƒå²›è·¯è·¯å£': 'åƒå²›è·¯',
                            'ä½“è‚²è·¯è·¯å£': 'ä½“è‚²è·¯',
                            'é‡‘å²›è·¯è·¯å£': 'é‡‘å²›è·¯',
                        }
                        
                        df['standardized_location'] = df['standardized_location'].replace(location_mapping)
                        
                        return df
                    
                    # çƒ­ç‚¹åˆ†æå‡½æ•°
                    def analyze_hotspot_frequency(df, time_window='7D'):
                        """åˆ†æåœ°ç‚¹äº‹æ•…é¢‘æ¬¡"""
                        df = standardize_hotspot_locations(df)
                        
                        # è®¡ç®—æ—¶é—´çª—å£
                        recent_cutoff = df['äº‹æ•…æ—¶é—´'].max() - pd.Timedelta(time_window)
                        
                        # æ€»ä½“ç»Ÿè®¡
                        overall_stats = df.groupby('standardized_location').agg({
                            'äº‹æ•…æ—¶é—´': ['count', 'max'],
                            'äº‹æ•…ç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'è´¢æŸ',
                            'é“è·¯ç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'åŸåŒºé“è·¯',
                            'è·¯å£è·¯æ®µç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'æ™®é€šè·¯æ®µ',
                            'severity': 'sum'
                        })
                        
                        # æ‰å¹³åŒ–åˆ—å
                        overall_stats.columns = ['accident_count', 'last_accident', 'main_accident_type', 
                                            'main_road_type', 'main_intersection_type', 'total_severity']
                        
                        # è¿‘æœŸç»Ÿè®¡
                        recent_accidents = df[df['äº‹æ•…æ—¶é—´'] >= recent_cutoff]
                        recent_stats = recent_accidents.groupby('standardized_location').agg({
                            'äº‹æ•…æ—¶é—´': 'count',
                            'äº‹æ•…ç±»å‹': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'è´¢æŸ',
                            'severity': 'sum'
                        }).rename(columns={'äº‹æ•…æ—¶é—´': 'recent_count', 'äº‹æ•…ç±»å‹': 'recent_accident_type', 'severity': 'recent_severity'})
                        
                        # åˆå¹¶æ•°æ®
                        result = overall_stats.merge(recent_stats, left_index=True, right_index=True, how='left').fillna(0)
                        result['recent_count'] = result['recent_count'].astype(int)
                        
                        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
                        result['trend_ratio'] = result['recent_count'] / result['accident_count']
                        result['days_since_last'] = (df['äº‹æ•…æ—¶é—´'].max() - result['last_accident']).dt.days
                        result['avg_severity'] = result['total_severity'] / result['accident_count']
                        
                        return result.sort_values(['recent_count', 'accident_count'], ascending=False)
                    
                    # é£é™©è¯„åˆ†å‡½æ•°
                    def calculate_hotspot_risk_score(hotspot_df):
                        """è®¡ç®—è·¯å£é£é™©è¯„åˆ†"""
                        df = hotspot_df.copy()
                        
                        # äº‹æ•…é¢‘æ¬¡å¾—åˆ† (0-40åˆ†)
                        df['frequency_score'] = (df['accident_count'] / df['accident_count'].max() * 40).clip(0, 40)
                        
                        # è¿‘æœŸè¶‹åŠ¿å¾—åˆ† (0-30åˆ†)
                        df['trend_score'] = (df['trend_ratio'] * 30).clip(0, 30)
                        
                        # äº‹æ•…ä¸¥é‡åº¦å¾—åˆ† (0-20åˆ†)
                        severity_map = {'è´¢æŸ': 5, 'ä¼¤äºº': 15, 'äº¡äºº': 20}
                        df['severity_score'] = df['main_accident_type'].map(severity_map).fillna(5)
                        
                        # æ—¶é—´ç´§è¿«åº¦å¾—åˆ† (0-10åˆ†)
                        df['urgency_score'] = ((30 - df['days_since_last']) / 30 * 10).clip(0, 10)
                        
                        # æ€»åˆ†
                        df['risk_score'] = df['frequency_score'] + df['trend_score'] + df['severity_score'] + df['urgency_score']
                        
                        # é£é™©ç­‰çº§
                        conditions = [
                            df['risk_score'] >= 70,
                            df['risk_score'] >= 50,
                            df['risk_score'] >= 30
                        ]
                        choices = ['é«˜é£é™©', 'ä¸­é£é™©', 'ä½é£é™©']
                        df['risk_level'] = np.select(conditions, choices, default='ä¸€èˆ¬é£é™©')
                        
                        return df.sort_values('risk_score', ascending=False)
                    
                    # ç­–ç•¥ç”Ÿæˆå‡½æ•°
                    def generate_hotspot_strategies(hotspot_df, time_period='æœ¬å‘¨'):
                        """ç”Ÿæˆçƒ­ç‚¹é’ˆå¯¹æ€§ç­–ç•¥"""
                        strategies = []
                        
                        for location_name, location_data in hotspot_df.iterrows():
                            accident_count = location_data['accident_count']
                            recent_count = location_data['recent_count']
                            accident_type = location_data['main_accident_type']
                            intersection_type = location_data['main_intersection_type']
                            trend_ratio = location_data['trend_ratio']
                            risk_level = location_data['risk_level']
                            
                            # åŸºç¡€ä¿¡æ¯
                            base_info = f"{time_period}å¯¹ã€{location_name}ã€‘"
                            data_support = f"ï¼ˆè¿‘æœŸ{int(recent_count)}èµ·ï¼Œç´¯è®¡{int(accident_count)}èµ·ï¼Œ{accident_type}ä¸ºä¸»ï¼‰"
                            
                            # æ™ºèƒ½ç­–ç•¥ç”Ÿæˆ
                            strategy_parts = []
                            
                            # åŸºäºè·¯å£ç±»å‹å’Œäº‹æ•…ç±»å‹
                            if 'ä¿¡å·ç¯' in str(intersection_type):
                                if accident_type == 'è´¢æŸ':
                                    strategy_parts.extend(["åŠ å¼ºé—¯çº¢ç¯æŸ¥å¤„", "ä¼˜åŒ–ä¿¡å·é…æ—¶", "æ•´æ²»ä¸æŒ‰è§„å®šè®©è¡Œ"])
                                else:
                                    strategy_parts.extend(["å®Œå–„äººè¡Œè¿‡è¡—è®¾æ–½", "åŠ å¼ºéæœºåŠ¨è½¦ç®¡ç†", "è®¾ç½®è­¦ç¤ºæ ‡å¿—"])
                            elif 'æ™®é€šè·¯æ®µ' in str(intersection_type):
                                strategy_parts.extend(["åŠ å¼ºå·¡é€»ç®¡æ§", "æ•´æ²»è¿æ³•åœè½¦", "è®¾ç½®é™é€Ÿæ ‡å¿—"])
                            else:
                                strategy_parts.extend(["åˆ†æäº‹æ•…æˆå› ", "åˆ¶å®šç»¼åˆæ•´æ²»æ–¹æ¡ˆ"])
                            
                            # åŸºäºé£é™©ç­‰çº§
                            if risk_level == 'é«˜é£é™©':
                                strategy_parts.append("åˆ—ä¸ºé‡ç‚¹æ•´æ²»è·¯æ®µ")
                                strategy_parts.append("å¼€å±•ä¸“é¡¹æ•´æ²»è¡ŒåŠ¨")
                            elif risk_level == 'ä¸­é£é™©':
                                strategy_parts.append("åŠ å¼ºæ—¥å¸¸ç›‘ç®¡")
                            
                            # åŸºäºè¶‹åŠ¿
                            if trend_ratio > 0.4:
                                strategy_parts.append("è¿‘æœŸé‡ç‚¹ç›‘æ§")
                            
                            # ç»„åˆç­–ç•¥
                            if strategy_parts:
                                strategy = base_info + "ï¼Œ" + "ï¼Œ".join(strategy_parts) + data_support
                            else:
                                strategy = base_info + "åŠ å¼ºäº¤é€šå®‰å…¨ç®¡ç†" + data_support
                            
                            strategies.append({
                                'location': location_name,
                                'strategy': strategy,
                                'risk_level': risk_level,
                                'accident_count': accident_count,
                                'recent_count': recent_count
                            })
                        
                        return strategies
                    
                    # åˆ†æå‚æ•°è®¾ç½®
                    st.subheader("ğŸ”§ åˆ†æå‚æ•°è®¾ç½®")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        time_window = st.selectbox("ç»Ÿè®¡æ—¶é—´çª—å£", ['7D', '15D', '30D'], index=0, key="hotspot_window")
                    with col2:
                        min_accidents = st.number_input("æœ€å°äº‹æ•…æ•°", 1, 50, 3, key="hotspot_min_accidents")
                    with col3:
                        top_n = st.slider("æ˜¾ç¤ºçƒ­ç‚¹æ•°é‡", 3, 20, 8, key="hotspot_top_n")
                    
                    if st.button("ğŸš€ å¼€å§‹çƒ­ç‚¹åˆ†æ", type="primary"):
                        with st.spinner("æ­£åœ¨åˆ†æäº‹æ•…çƒ­ç‚¹åˆ†å¸ƒ..."):
                            # æ‰§è¡Œçƒ­ç‚¹åˆ†æ
                            hotspots = analyze_hotspot_frequency(accident_data, time_window=time_window)
                            
                            # è¿‡æ»¤æœ€å°äº‹æ•…æ•°
                            hotspots = hotspots[hotspots['accident_count'] >= min_accidents]
                            
                            if len(hotspots) > 0:
                                # è®¡ç®—é£é™©è¯„åˆ†
                                hotspots_with_risk = calculate_hotspot_risk_score(hotspots.head(top_n * 3))
                                top_hotspots = hotspots_with_risk.head(top_n)
                                
                                # æ˜¾ç¤ºçƒ­ç‚¹æ’å
                                st.subheader(f"ğŸ“Š äº‹æ•…å¤šå‘è·¯å£æ’åï¼ˆå‰{top_n}ä¸ªï¼‰")
                                
                                display_df = top_hotspots[[
                                    'accident_count', 'recent_count', 'trend_ratio', 
                                    'main_accident_type', 'main_intersection_type', 'risk_score', 'risk_level'
                                ]].rename(columns={
                                    'accident_count': 'ç´¯è®¡äº‹æ•…',
                                    'recent_count': 'è¿‘æœŸäº‹æ•…',
                                    'trend_ratio': 'è¶‹åŠ¿æ¯”ä¾‹',
                                    'main_accident_type': 'ä¸»è¦ç±»å‹',
                                    'main_intersection_type': 'è·¯å£ç±»å‹',
                                    'risk_score': 'é£é™©è¯„åˆ†',
                                    'risk_level': 'é£é™©ç­‰çº§'
                                })
                                
                                # æ ¼å¼åŒ–æ˜¾ç¤º
                                styled_df = display_df.style.format({
                                    'è¶‹åŠ¿æ¯”ä¾‹': '{:.2f}',
                                    'é£é™©è¯„åˆ†': '{:.1f}'
                                }).background_gradient(subset=['é£é™©è¯„åˆ†'], cmap='Reds')
                                
                                st.dataframe(styled_df, use_container_width=True)
                                
                                # ç”Ÿæˆç­–ç•¥å»ºè®®
                                strategies = generate_hotspot_strategies(top_hotspots, time_period='æœ¬å‘¨')
                                
                                st.subheader("ğŸ¯ é’ˆå¯¹æ€§ç­–ç•¥å»ºè®®")
                                
                                for i, strategy_info in enumerate(strategies, 1):
                                    strategy = strategy_info['strategy']
                                    risk_level = strategy_info['risk_level']
                                    
                                    # æ ¹æ®é£é™©ç­‰çº§æ˜¾ç¤ºä¸åŒé¢œè‰²
                                    if risk_level == 'é«˜é£é™©':
                                        st.error(f"ğŸš¨ **{i}. {strategy}**")
                                    elif risk_level == 'ä¸­é£é™©':
                                        st.warning(f"âš ï¸ **{i}. {strategy}**")
                                    else:
                                        st.info(f"âœ… **{i}. {strategy}**")
                                
                                # å¯è§†åŒ–åˆ†æ
                                st.subheader("ğŸ“ˆ æ•°æ®åˆ†æå¯è§†åŒ–")
                                
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    # äº‹æ•…é¢‘æ¬¡åˆ†å¸ƒå›¾
                                    fig1 = px.bar(
                                        top_hotspots.head(10),
                                        x=top_hotspots.head(10).index,
                                        y=['accident_count', 'recent_count'],
                                        title="äº‹æ•…é¢‘æ¬¡TOP10åˆ†å¸ƒ",
                                        labels={'value': 'äº‹æ•…æ•°é‡', 'variable': 'ç±»å‹', 'index': 'è·¯å£åç§°'},
                                        barmode='group'
                                    )
                                    fig1.update_layout(xaxis_tickangle=-45)
                                    st.plotly_chart(fig1, use_container_width=True)
                                
                                with col2:
                                    # é£é™©ç­‰çº§åˆ†å¸ƒ
                                    risk_dist = top_hotspots['risk_level'].value_counts()
                                    fig2 = px.pie(
                                        values=risk_dist.values,
                                        names=risk_dist.index,
                                        title="é£é™©ç­‰çº§åˆ†å¸ƒ",
                                        color_discrete_map={'é«˜é£é™©': 'red', 'ä¸­é£é™©': 'orange', 'ä½é£é™©': 'green'}
                                    )
                                    st.plotly_chart(fig2, use_container_width=True)
                                
                                # è¯¦ç»†æ•°æ®ä¸‹è½½
                                st.subheader("ğŸ’¾ æ•°æ®å¯¼å‡º")
                                
                                col_dl1, col_dl2 = st.columns(2)
                                
                                with col_dl1:
                                    # ä¸‹è½½çƒ­ç‚¹æ•°æ®
                                    hotspot_csv = top_hotspots.to_csv().encode('utf-8-sig')
                                    st.download_button(
                                        "ğŸ“¥ ä¸‹è½½çƒ­ç‚¹æ•°æ®CSV",
                                        data=hotspot_csv,
                                        file_name=f"accident_hotspots_{datetime.now().strftime('%Y%m%d')}.csv",
                                        mime="text/csv"
                                    )
                                
                                with col_dl2:
                                    # ä¸‹è½½ç­–ç•¥æŠ¥å‘Š
                                    report_data = {
                                        "analysis_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                        "time_window": time_window,
                                        "data_source": hotspot_file.name,
                                        "total_records": len(accident_data),
                                        "analysis_parameters": {
                                            "min_accidents": min_accidents,
                                            "top_n": top_n
                                        },
                                        "top_hotspots": top_hotspots.to_dict('records'),
                                        "recommended_strategies": strategies,
                                        "summary": {
                                            "high_risk_count": len(top_hotspots[top_hotspots['risk_level'] == 'é«˜é£é™©']),
                                            "medium_risk_count": len(top_hotspots[top_hotspots['risk_level'] == 'ä¸­é£é™©']),
                                            "total_analyzed_locations": len(hotspots),
                                            "most_dangerous_location": top_hotspots.index[0] if len(top_hotspots) > 0 else "æ— "
                                        }
                                    }
                                    
                                    st.download_button(
                                        "ğŸ“„ ä¸‹è½½å®Œæ•´åˆ†ææŠ¥å‘Š",
                                        data=json.dumps(report_data, ensure_ascii=False, indent=2),
                                        file_name=f"hotspot_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
                                        mime="application/json"
                                    )
                                
                            else:
                                st.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº‹æ•…çƒ­ç‚¹æ•°æ®ï¼Œè¯·è°ƒæ•´ç­›é€‰å‚æ•°")
                    
                    # æ˜¾ç¤ºåŸå§‹æ•°æ®é¢„è§ˆï¼ˆå¯é€‰ï¼‰
                    with st.expander("ğŸ“‹ æŸ¥çœ‹åŸå§‹æ•°æ®é¢„è§ˆ"):
                        st.dataframe(accident_data[['äº‹æ•…æ—¶é—´', 'æ‰€åœ¨è¡—é“', 'äº‹æ•…ç±»å‹', 'äº‹æ•…å…·ä½“åœ°ç‚¹', 'é“è·¯ç±»å‹']].head(10), 
                                use_container_width=True)
                        
                except Exception as e:
                    st.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}")
                    st.info("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿åŒ…å«'äº‹æ•…æ—¶é—´'ã€'äº‹æ•…ç±»å‹'ã€'äº‹æ•…å…·ä½“åœ°ç‚¹'ç­‰å¿…è¦å­—æ®µ")
            
            else:
                st.info("ğŸ‘† è¯·ä¸Šä¼ äº‹æ•…æ•°æ®Excelæ–‡ä»¶å¼€å§‹åˆ†æ")
                st.markdown("""
                ### ğŸ“ æ”¯æŒçš„æ•°æ®æ ¼å¼è¦æ±‚ï¼š
                - **æ–‡ä»¶æ ¼å¼**: Excel (.xlsx)
                - **å¿…è¦å­—æ®µ**:
                - `äº‹æ•…æ—¶é—´`: äº‹æ•…å‘ç”Ÿæ—¶çš„æ—¶é—´
                - `äº‹æ•…ç±»å‹`: è´¢æŸ/ä¼¤äºº/äº¡äºº
                - `äº‹æ•…å…·ä½“åœ°ç‚¹`: è¯¦ç»†çš„äº‹æ•…å‘ç”Ÿåœ°ç‚¹
                - `æ‰€åœ¨è¡—é“`: äº‹æ•…å‘ç”Ÿçš„è¡—é“åŒºåŸŸ
                - `é“è·¯ç±»å‹`: åŸåŒºé“è·¯/å…¶ä»–ç­‰
                - `è·¯å£è·¯æ®µç±»å‹`: ä¿¡å·ç¯è·¯å£/æ™®é€šè·¯æ®µç­‰
                """)
        # --- Tab 1: æ€»è§ˆé¡µ
        with tab_dash:
            fig_line = go.Figure()
            fig_line.add_trace(go.Scatter(x=base.index, y=base['accident_count'], name='äº‹æ•…æ•°', mode='lines'))
            fig_line.update_layout(title="äº‹æ•…æ•°ï¼ˆè¿‡æ»¤åï¼‰", xaxis_title="Date", yaxis_title="Count")
            st.plotly_chart(fig_line, use_container_width=True)
            fname = save_fig_as_html(fig_line, "overview_series.html")
            st.download_button("ä¸‹è½½å›¾è¡¨ HTML", data=open(fname, 'rb').read(),
                               file_name="overview_series.html", mime="text/html")

            st.dataframe(base, use_container_width=True)
            csv_bytes = base.to_csv(index=True).encode('utf-8-sig')
            st.download_button("ä¸‹è½½å½“å‰è§†å›¾ CSV", data=csv_bytes, file_name="filtered_view.csv", mime="text/csv")

            meta = {
                "region": region_sel,
                "date_range": [str(start_dt.date()), str(end_dt.date())],
                "strategy_filter": strat_filter,
                "rows": int(len(base)),
                "min_date": str(base.index.min().date()) if len(base) else None,
                "max_date": str(base.index.max().date()) if len(base) else None
            }
            with open("run_metadata.json", "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
            st.download_button("ä¸‹è½½è¿è¡Œå‚æ•° JSON", data=open("run_metadata.json", "rb").read(),
                               file_name="run_metadata.json", mime="application/json")

        # --- Tab 2: é¢„æµ‹æ¨¡å‹
        with tab_pred:
            st.subheader("å¤šæ¨¡å‹é¢„æµ‹æ¯”è¾ƒ")
            # ä½¿ç”¨è¡¨å•å°è£…äº¤äº’ç»„ä»¶
            with st.form(key="predict_form"):
                default_date = base.index.max() - pd.Timedelta(days=60) if len(base) else pd.Timestamp('2022-01-01')
                selected_date = st.date_input("é€‰æ‹©å¹²é¢„æ—¥æœŸ / é¢„æµ‹èµ·ç‚¹", value=default_date)
                horizon = st.number_input("é¢„æµ‹å¤©æ•°", min_value=7, max_value=90, value=30, step=1)
                submit_predict = st.form_submit_button("åº”ç”¨é¢„æµ‹å‚æ•°")

            if submit_predict and len(base.loc[:pd.to_datetime(selected_date)]) >= 10:
                first_date = pd.to_datetime(selected_date)
                try:
                    train_series = base['accident_count'].loc[:first_date]
                    arima30 = arima_forecast_with_grid_search(
                        train_series,
                        start_date=first_date + pd.Timedelta(days=1),
                        horizon=horizon
                    )
                except Exception as e:
                    st.warning(f"ARIMA è¿è¡Œå¤±è´¥ï¼š{e}")
                    arima30 = None

                knn_pred, _ = knn_forecast_counterfactual(base['accident_count'],
                                                        first_date,
                                                        horizon=horizon)
                glm_pred, svr_pred, residuals = fit_and_extrapolate(base['accident_count'],
                                                                    first_date,
                                                                    days=horizon)

                fig_pred = go.Figure()
                fig_pred.add_trace(go.Scatter(x=base.index, y=base['accident_count'],
                                            name="å®é™…", mode="lines"))
                if arima30 is not None:
                    fig_pred.add_trace(go.Scatter(x=arima30.index, y=arima30['forecast'],
                                                name="ARIMA", mode="lines"))
                if knn_pred is not None:
                    fig_pred.add_trace(go.Scatter(x=knn_pred.index, y=knn_pred,
                                                name="KNN", mode="lines"))
                if glm_pred is not None:
                    fig_pred.add_trace(go.Scatter(x=glm_pred.index, y=glm_pred,
                                                name="GLM", mode="lines"))
                if svr_pred is not None:
                    fig_pred.add_trace(go.Scatter(x=svr_pred.index, y=svr_pred,
                                                name="SVR", mode="lines"))

                fig_pred.update_layout(
                    title=f"å¤šæ¨¡å‹é¢„æµ‹æ¯”è¾ƒï¼ˆèµ·ç‚¹ï¼š{first_date.date()}ï¼Œé¢„æµ‹ {horizon} å¤©ï¼‰",
                    xaxis_title="æ—¥æœŸ", yaxis_title="äº‹æ•…æ•°"
                )
                st.plotly_chart(fig_pred, use_container_width=True)

                col_dl1, col_dl2 = st.columns(2)
                if arima30 is not None:
                    col_dl1.download_button("ä¸‹è½½ ARIMA é¢„æµ‹ CSV",
                                        data=arima30.to_csv().encode("utf-8-sig"),
                                        file_name="arima_forecast.csv",
                                        mime="text/csv")
            elif submit_predict:
                st.info("âš ï¸ å¹²é¢„å‰æ•°æ®è¾ƒå°‘ï¼Œå¯èƒ½å½±å“æ‹Ÿåˆè´¨é‡ã€‚")
            else:
                st.info("è¯·è®¾ç½®é¢„æµ‹å‚æ•°å¹¶ç‚¹å‡»â€œåº”ç”¨é¢„æµ‹å‚æ•°â€æŒ‰é’®ã€‚")

        # --- Tab 3: æ¨¡å‹è¯„ä¼°
        with tab_eval:
            st.subheader("æ¨¡å‹é¢„æµ‹æ•ˆæœå¯¹æ¯”")
            with st.form(key="model_eval_form"):
                horizon_sel = st.slider("è¯„ä¼°çª—å£ï¼ˆå¤©ï¼‰", 7, 60, 30, step=1)
                submit_eval = st.form_submit_button("åº”ç”¨è¯„ä¼°å‚æ•°")

            if submit_eval:
                try:
                    df_metrics = evaluate_models(base['accident_count'], horizon=horizon_sel)
                    st.dataframe(df_metrics, use_container_width=True)
                    best_model = df_metrics['RMSE'].idxmin()
                    st.success(f"è¿‡å» {horizon_sel} å¤©ä¸­ï¼ŒRMSE æœ€ä½çš„æ¨¡å‹æ˜¯ï¼š**{best_model}**")
                    st.download_button(
                        "ä¸‹è½½è¯„ä¼°ç»“æœ CSV",
                        data=df_metrics.to_csv().encode('utf-8-sig'),
                        file_name="model_evaluation.csv",
                        mime="text/csv"
                    )
                except ValueError as err:
                    st.warning(str(err))
            else:
                st.info("è¯·è®¾ç½®è¯„ä¼°çª—å£å¹¶ç‚¹å‡»â€œåº”ç”¨è¯„ä¼°å‚æ•°â€æŒ‰é’®ã€‚")

        # --- Tab 4: å¼‚å¸¸æ£€æµ‹
        with tab_anom:
            anomalies, anomaly_fig = detect_anomalies(base['accident_count'])
            st.plotly_chart(anomaly_fig, use_container_width=True)
            st.write(f"æ£€æµ‹åˆ°å¼‚å¸¸ç‚¹ï¼š{len(anomalies)} ä¸ª")
            st.download_button("ä¸‹è½½å¼‚å¸¸æ—¥æœŸ CSV",
                            data=anomalies.to_series().to_csv(index=False).encode('utf-8-sig'),
                            file_name="anomalies.csv", mime="text/csv")

        # --- Tab 5: ç­–ç•¥è¯„ä¼°
        with tab_strat:
            st.info(f"ğŸ“Œ æ£€æµ‹åˆ°çš„ç­–ç•¥ç±»å‹ï¼š{', '.join(all_strategy_types) or 'ï¼ˆæ•°æ®ä¸­æ²¡æœ‰ç­–ç•¥ï¼‰'}")
            if all_strategy_types:
                results, recommendation = generate_output_and_recommendations(base, all_strategy_types,
                                                                              region=region_sel if region_sel!='å…¨å¸‚' else 'å…¨å¸‚')
                st.subheader("å„ç­–ç•¥æŒ‡æ ‡")
                df_res = pd.DataFrame(results).T
                st.dataframe(df_res, use_container_width=True)
                st.success(f"â­ æ¨èï¼š{recommendation}")
                st.download_button("ä¸‹è½½ç­–ç•¥è¯„ä¼° CSV",
                                   data=df_res.to_csv().encode('utf-8-sig'),
                                   file_name="strategy_evaluation_results.csv", mime="text/csv")
                with open('recommendation.txt','r',encoding='utf-8') as f:
                    st.download_button("ä¸‹è½½æ¨èæ–‡æœ¬", data=f.read().encode('utf-8'), file_name="recommendation.txt")
            else:
                st.warning("æ•°æ®ä¸­æ²¡æœ‰æ£€æµ‹åˆ°ç­–ç•¥ã€‚")

        # --- Tab 6: ç­–ç•¥å¯¹æ¯”
        with tab_comp:
            def strategy_metrics(strategy):
                mask = base['strategy_type'].apply(lambda x: strategy in x)
                if not mask.any():
                    return None
                dt = mask[mask].index[0]
                glm_pred, svr_pred, residuals = fit_and_extrapolate(base['accident_count'], dt, days=30)
                if svr_pred is None:
                    return None
                actual_post = base['accident_count'].loc[dt:dt+pd.Timedelta(days=29)]
                pre = base['accident_count'].loc[dt-pd.Timedelta(days=30):dt-pd.Timedelta(days=1)]
                stat, p = significance_test(pre, actual_post)
                count_eff, sev_eff, (F1, F2), state = evaluate_strategy_effectiveness(
                    actual_series=base['accident_count'],
                    counterfactual_series=svr_pred,
                    severity_series=base['severity'],
                    strategy_date=dt, window=30
                )
                return {
                    "å¹²é¢„æ—¥": str(dt.date()),
                    "å‰30å¤©äº‹æ•…": int(pre.sum()),
                    "å30å¤©äº‹æ•…": int(actual_post.sum()),
                    "æ¯æ—¥å‡å€¼(å‰/å)": (float(pre.mean()), float(actual_post.mean())),
                    "tç»Ÿè®¡/på€¼": (stat, p),
                    "F1/F2": (float(F1), float(F2)),
                    "æœ‰æ•ˆå¤©æ•°è¿‡åŠ?": bool(count_eff),
                    "ä¸¥é‡åº¦ä¸‹é™?": bool(sev_eff),
                    "å®‰å…¨ç­‰çº§": state
                }
            if all_strategy_types:
                st.subheader("ç­–ç•¥å¯¹æ¯”")
                with st.form(key="strategy_compare_form"):
                    colA, colB = st.columns(2)
                    with colA:
                        sA = st.selectbox("ç­–ç•¥ A", options=all_strategy_types, key="stratA")
                    with colB:
                        sB = st.selectbox("ç­–ç•¥ B", options=[s for s in all_strategy_types if s != st.session_state.get("stratA")], key="stratB")
                    submit_compare = st.form_submit_button("åº”ç”¨ç­–ç•¥å¯¹æ¯”")

                if submit_compare:
                    mA = strategy_metrics(sA)
                    mB = strategy_metrics(sB)
                    if mA and mB:
                        show = pd.DataFrame({
                            "æŒ‡æ ‡": ["å¹²é¢„æ—¥", "å‰30å¤©äº‹æ•…", "å30å¤©äº‹æ•…", "æ¯æ—¥å‡å€¼(å‰)", "æ¯æ—¥å‡å€¼(å)", "tç»Ÿè®¡", "på€¼", "F1", "F2", "æœ‰æ•ˆå¤©æ•°è¿‡åŠ?", "ä¸¥é‡åº¦ä¸‹é™?", "å®‰å…¨ç­‰çº§"],
                            f"{sA}": [mA["å¹²é¢„æ—¥"], mA["å‰30å¤©äº‹æ•…"], mA["å30å¤©äº‹æ•…"],
                                    mA["æ¯æ—¥å‡å€¼(å‰/å)"][0], mA["æ¯æ—¥å‡å€¼(å‰/å)"][1],
                                    mA["tç»Ÿè®¡/på€¼"][0], mA["tç»Ÿè®¡/på€¼"][1],
                                    mA["F1/F2"][0], mA["F1/F2"][1],
                                    mA["æœ‰æ•ˆå¤©æ•°è¿‡åŠ?"], mA["ä¸¥é‡åº¦ä¸‹é™?"], mA["å®‰å…¨ç­‰çº§"]],
                            f"{sB}": [mB["å¹²é¢„æ—¥"], mB["å‰30å¤©äº‹æ•…"], mB["å30å¤©äº‹æ•…"],
                                    mB["æ¯æ—¥å‡å€¼(å‰/å)"][0], mB["æ¯æ—¥å‡å€¼(å‰/å)"][1],
                                    mB["tç»Ÿè®¡/på€¼"][0], mB["tç»Ÿè®¡/på€¼"][1],
                                    mB["F1/F2"][0], mB["F1/F2"][1],
                                    mB["æœ‰æ•ˆå¤©æ•°è¿‡åŠ?"], mB["ä¸¥é‡åº¦ä¸‹é™?"], mB["å®‰å…¨ç­‰çº§"]],
                        })
                        st.dataframe(show, use_container_width=True)
                        st.download_button("ä¸‹è½½å¯¹æ¯”è¡¨ CSV",
                                        data=show.to_csv(index=False).encode('utf-8-sig'),
                                        file_name="strategy_compare.csv", mime="text/csv")
                    else:
                        st.info("æ‰€é€‰ç­–ç•¥å¯èƒ½ç¼ºå°‘è¶³å¤Ÿçš„å¹²é¢„å‰æ•°æ®æˆ–æœªåœ¨å½“å‰è¿‡æ»¤èŒƒå›´å†…å‡ºç°ã€‚")
                else:
                    st.info("è¯·é€‰æ‹©ç­–ç•¥å¹¶ç‚¹å‡»â€œåº”ç”¨ç­–ç•¥å¯¹æ¯”â€æŒ‰é’®ã€‚")
            else:
                st.warning("æ²¡æœ‰ç­–ç•¥å¯ä¾›å¯¹æ¯”ã€‚")

        # --- Tab 7: æƒ…æ™¯æ¨¡æ‹Ÿ
        with tab_sim:
            st.subheader("æƒ…æ™¯æ¨¡æ‹Ÿ")
            st.write("é€‰æ‹©ä¸€ä¸ªæ—¥æœŸä¸ç­–ç•¥ï¼Œæ¨¡æ‹Ÿâ€œåœ¨è¯¥æ—¥æœŸä¸Šçº¿è¯¥ç­–ç•¥â€çš„å½±å“ï¼š")
            with st.form(key="simulation_form"):
                sim_date = st.date_input("æ¨¡æ‹Ÿç­–ç•¥ä¸Šçº¿æ—¥æœŸ", value=(base.index.max() - pd.Timedelta(days=14)))
                sim_strategy = st.selectbox("æ¨¡æ‹Ÿç­–ç•¥ç±»å‹", options=all_strategy_types or ["ç¤ºä¾‹ç­–ç•¥"])
                sim_days = st.slider("æ¨¡æ‹Ÿå¤©æ•°", 7, 60, 30)
                submit_simulation = st.form_submit_button("åº”ç”¨æ¨¡æ‹Ÿå‚æ•°")

            if submit_simulation:
                glm_pred, svr_pred, residuals = fit_and_extrapolate(base['accident_count'], pd.to_datetime(sim_date), days=sim_days)
                if svr_pred is None:
                    st.warning("å¹²é¢„å‰æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ¨¡æ‹Ÿã€‚")
                else:
                    count_eff, sev_eff, (F1, F2), state = evaluate_strategy_effectiveness(
                        actual_series=base['accident_count'],
                        counterfactual_series=svr_pred,
                        severity_series=base['severity'],
                        strategy_date=pd.to_datetime(sim_date),
                        window=sim_days
                    )
                    fig_sim = go.Figure()
                    fig_sim.add_trace(go.Scatter(x=base.index, y=base['accident_count'], name='å®é™…', mode='lines'))
                    fig_sim.add_trace(go.Scatter(x=svr_pred.index, y=svr_pred, name='Counterfactual(SVR)', mode='lines'))
                    fig_sim.update_layout(title=f"æƒ…æ™¯æ¨¡æ‹Ÿï¼š{sim_strategy} è‡ª {sim_date} èµ·", xaxis_title="æ—¥æœŸ", yaxis_title="äº‹æ•…æ•°")
                    st.plotly_chart(fig_sim, use_container_width=True)

                    st.success(f"æ¨¡æ‹Ÿç»“æœï¼šF1={F1:.2f}, F2={F2:.2f}, ç­‰çº§={state}ï¼›"
                            f"{'äº‹æ•…æ•°åœ¨å¤šæ•°å¤©å°äºcounterfactual' if count_eff else 'æ•ˆæœä¸æ˜æ˜¾'}ï¼›"
                            f"{'ä¸¥é‡åº¦ä¸‹é™' if sev_eff else 'ä¸¥é‡åº¦æ— ä¸‹é™'}ã€‚")
                    st.download_button("ä¸‹è½½æ¨¡æ‹Ÿå›¾ HTML",
                                    data=open(save_fig_as_html(fig_sim, "simulation.html"), "rb").read(),
                                    file_name="simulation.html", mime="text/html")
            else:
                st.info("è¯·è®¾ç½®æ¨¡æ‹Ÿå‚æ•°å¹¶ç‚¹å‡»â€œåº”ç”¨æ¨¡æ‹Ÿå‚æ•°â€æŒ‰é’®ã€‚")

        # --- New Tab 8: GPT åˆ†æ
        with tab_gpt:
            from openai import OpenAI
            st.subheader("GPT æ•°æ®åˆ†æä¸æ”¹è¿›å»ºè®®")
            # open_ai_key = f"sk-dQhKOOG48iVEfgJfAb14458dA4474fB09aBbE8153d4aB3Fc"
            if not HAS_OPENAI:
                st.warning("æœªå®‰è£… `openai` åº“ã€‚è¯·å®‰è£…åé‡è¯•ã€‚")
            elif not openai_api_key:
                st.info("è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥ OpenAI API Key ä»¥å¯ç”¨ GPT åˆ†æã€‚")
            else:
                if all_strategy_types:
                    # Generate results if not already
                    results, recommendation = generate_output_and_recommendations(base, all_strategy_types,
                                                                                  region=region_sel if region_sel != 'å…¨å¸‚' else 'å…¨å¸‚')
                    df_res = pd.DataFrame(results).T
                    kpi_json = json.dumps(kpi, ensure_ascii=False, indent=2)
                    results_json = df_res.to_json(orient="records", force_ascii=False)
                    recommendation_text = recommendation

                    # Prepare data to send
                    data_to_analyze = {
                        "kpis": kpi_json,
                        "strategy_results": results_json,
                        "recommendation": recommendation_text
                    }
                    data_str = json.dumps(data_to_analyze, ensure_ascii=False)

                    prompt = str(f"""
                    è¯·åˆ†æä»¥ä¸‹äº¤é€šå®‰å…¨åˆ†æç»“æœï¼ŒåŒ…æ‹¬KPIæŒ‡æ ‡ã€ç­–ç•¥è¯„ä¼°ç»“æœå’Œæ¨èã€‚
                    æä¾›æ•°æ®ç»“æœçš„è¯¦ç»†åˆ†æï¼Œä»¥åŠæ”¹è¿›æ€è·¯å’Œå»ºè®®ã€‚
                    æ•°æ®ï¼š{str(data_str)}
                    """)
                    #st.text_area(prompt)
                    if st.button("ä¸Šä¼ æ•°æ®è‡³ GPT å¹¶è·å–åˆ†æ"):
                        try:
                            client = OpenAI(
                                    base_url=open_ai_base_url,
                                    # sk-xxxæ›¿æ¢ä¸ºè‡ªå·±çš„key
                                    api_key=openai_api_key
                            )
                            response = client.chat.completions.create(
                                model="gpt-4o",
                                messages=[
                                    {"role": "system", "content": "You are a helpful assistant that analyzes traffic safety data."},
                                    {"role": "user", "content": prompt}
                                ],
                                stream=False
                            )
                            gpt_response = response.choices[0].message.content 
                            st.markdown("### GPT åˆ†æç»“æœä¸æ”¹è¿›æ€è·¯")
                            st.markdown(gpt_response, unsafe_allow_html=True)
                        except Exception as e:
                            st.error(f"è°ƒç”¨ OpenAI API å¤±è´¥ï¼š{str(e)}")
                else:
                    st.warning("æ²¡æœ‰ç­–ç•¥æ•°æ®å¯ä¾›åˆ†æã€‚")

                # Update refresh time
                st.session_state['last_refresh'] = datetime.now()

    else:
        st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ äº‹æ•…æ•°æ®ä¸ç­–ç•¥æ•°æ®ï¼Œå¹¶ç‚¹å‡»â€œåº”ç”¨æ•°æ®ä¸ç­›é€‰â€æŒ‰é’®ã€‚")

if __name__ == "__main__":
    run_streamlit_app()